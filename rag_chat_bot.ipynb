{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5123a0a",
   "metadata": {},
   "source": [
    "# Project : Customer‚ÄëSupport Chatbot for an E-Commerce Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafbc009",
   "metadata": {},
   "source": [
    "## Roadmap  \n",
    "We will build a RAG-based chatbot in **six** steps:\n",
    "\n",
    "1. **Environment setup**\n",
    "2. **Data preparation**  \n",
    "   a. Load source documents  \n",
    "   b. Chunk the text  \n",
    "3. **Build a retriever**  \n",
    "   a. Generate embeddings  \n",
    "   b. Build the FAISS vector index  \n",
    "4. **Build a generation engine**. Load the *Gemma3-1B* model through Ollama and run a sanity check.  \n",
    "5. **Build a RAG**. Connect the system prompt, retriever, and LLM together. \n",
    "6. **(Optional) Streamlit UI**. Wrap everything in a simple web app so users can chat with the bot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ee13f5",
   "metadata": {},
   "source": [
    "## 1‚ÄØ-‚ÄØEnvironment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b57da2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/farjam/miniconda3/envs/llm_demo_projects/lib/python3.11/site-packages/requests/__init__.py:113: RequestsDependencyWarning: urllib3 (2.6.3) or chardet (6.0.0.post1)/charset_normalizer (3.4.4) doesn't match a supported version!\n",
      "  warnings.warn(\n",
      "/home/farjam/miniconda3/envs/llm_demo_projects/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported - you're good to go!\n"
     ]
    }
   ],
   "source": [
    "# Import standard libraries for file handling and text processing\n",
    "import os, pathlib, textwrap, glob\n",
    "\n",
    "# Load documents from various sources (URLs, text files, PDFs)\n",
    "from langchain_community.document_loaders import UnstructuredURLLoader, TextLoader, PyPDFLoader\n",
    "\n",
    "# Split long texts into smaller, manageable chunks for embedding\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Vector store to store and retrieve embeddings efficiently using FAISS\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Generate text embeddings using OpenAI or Hugging Face models\n",
    "from langchain.embeddings import OpenAIEmbeddings, HuggingFaceEmbeddings, SentenceTransformerEmbeddings\n",
    "\n",
    "# Use local LLMs (e.g., via Ollama) for response generation\n",
    "from langchain.llms import Ollama\n",
    "\n",
    "# Build a retrieval chain that combines a retriever, a prompt, and an LLM\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "# Create prompts for the RAG system\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "print(\"‚úÖ Libraries imported - you're good to go!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553ae6b7",
   "metadata": {},
   "source": [
    "## 2‚ÄØ-‚ÄØData preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "603f722d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 81 0 (offset 0)\n",
      "Ignoring wrong pointing object 80 0 (offset 0)\n",
      "Ignoring wrong pointing object 76 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8 PDF pages from 4 files.\n"
     ]
    }
   ],
   "source": [
    "pdf_paths = glob.glob(\"data/Everstorm_*.pdf\")\n",
    "\n",
    "raw_docs = []\n",
    "for path in pdf_paths:\n",
    "    raw_docs.extend(PyPDFLoader(path).load())\n",
    "\n",
    "print(f\"Loaded {len(raw_docs)} PDF pages from {len(pdf_paths)} files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9372ec21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLS = [\n",
    "#     # --- BigCommerce ‚Äì shipping & refunds ---\n",
    "#     \"https://developer.bigcommerce.com/docs/store-operations/shipping\",\n",
    "#     \"https://developer.bigcommerce.com/docs/store-operations/orders/refunds\",\n",
    "#     # --- Stripe ‚Äì disputes & chargebacks ---\n",
    "#     # \"https://docs.stripe.com/disputes\",  \n",
    "#     # --- WooCommerce ‚Äì REST API reference ---\n",
    "#     # \"https://woocommerce.github.io/woocommerce-rest-api-docs/v3.html\",\n",
    "# ]\n",
    "\n",
    "# try:\n",
    "#     from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "#     loader = WebBaseLoader(URLS)\n",
    "#     raw_docs = loader.load()\n",
    "#     print(f\"Fetched {len(raw_docs)} documents from the web.\")\n",
    "# except Exception as e:\n",
    "#     print(\"‚ö†Ô∏è  Web fetch failed, using offline copies:\", e)\n",
    "#     raw_docs = []\n",
    "\n",
    "#     from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "\n",
    "#     # Put any fallback files under ./offline_docs (e.g., .md, .txt, .html)\n",
    "#     patterns = [\"**/*.md\", \"**/*.txt\", \"**/*.html\"]\n",
    "#     for pattern in patterns:\n",
    "#         try:\n",
    "#             loader = DirectoryLoader(\n",
    "#                 \"offline_docs\",\n",
    "#                 glob=pattern,\n",
    "#                 loader_cls=TextLoader,\n",
    "#                 show_progress=True,\n",
    "#                 use_multithreading=True,\n",
    "#             )\n",
    "#             raw_docs.extend(loader.load())\n",
    "#         except Exception:\n",
    "#             # Skip unreadable pattern/file types quietly\n",
    "#             pass\n",
    "\n",
    "#     print(f\"Loaded {len(raw_docs)} offline documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccc0813",
   "metadata": {},
   "source": [
    "### 2.1‚ÄØ-‚ÄØChunk the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aaf7ca10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 42 chunks ready for embedding\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=30)\n",
    "chunks = text_splitter.split_documents(raw_docs)\n",
    "print(f\"‚úÖ {len(chunks)} chunks ready for embedding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebda6667",
   "metadata": {},
   "source": [
    "## 3¬†-Build a retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d5cced6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9471/34837372.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = SentenceTransformerEmbeddings(model_name=\"thenlper/gte-small\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    }
   ],
   "source": [
    "embeddings = SentenceTransformerEmbeddings(model_name=\"thenlper/gte-small\")\n",
    "\n",
    "# Single embedding for one sentence\n",
    "embedding_vector = embeddings.embed_query(\"What is the capital of France?\")\n",
    "print(len(embedding_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d123ad2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Vector store with 42 embeddings\n"
     ]
    }
   ],
   "source": [
    "vectordb = FAISS.from_documents(chunks, embeddings)\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 8})\n",
    "\n",
    "vectordb.save_local(\"faiss_index\")\n",
    "\n",
    "print(\"‚úÖ Vector store with\", vectordb.index.ntotal, \"embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c739151",
   "metadata": {},
   "source": [
    "## 4¬†-¬†Build the generation engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "796298ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9471/4067377706.py:1: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"gemma3:1b\", temperature=0.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM loaded: \u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'gemma3:1b', 'format': None, 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': None, 'num_thread': None, 'num_predict': None, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': 0.1, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None, 'raw': None}\n",
      "The capital of France is **Paris**. \n",
      "\n",
      "It‚Äôs a very popular and important question! üòä \n",
      "\n",
      "Do you want to know more about Paris, like its history or famous landmarks?\n"
     ]
    }
   ],
   "source": [
    "llm = Ollama(model=\"gemma3:1b\", temperature=0.1)\n",
    "print(\"‚úÖ LLM loaded:\", llm)\n",
    "print(llm.invoke(\"What is the capital of France?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb79a4e",
   "metadata": {},
   "source": [
    "## Build a RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f313f98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_TEMPLATE = \"\"\"\n",
    "You are a **Customer Support Chatbot**. Use only the information in CONTEXT to answer.\n",
    "If the answer is not in CONTEXT, respond with ‚ÄúI'm not sure from the docs.‚Äù\n",
    "\n",
    "Rules:\n",
    "1) Use ONLY the provided <context> to answer.\n",
    "2) If the answer is not in the context, say: \"I don't know based on the retrieved documents.\"\n",
    "3) Be concise and accurate. Prefer quoting key phrases from the context.\n",
    "4) When possible, cite sources as [source: source] using the metadata.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "USER:\n",
    "{question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da26803c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=SYSTEM_TEMPLATE,\n",
    ")\n",
    "\n",
    "llm = Ollama(model=\"gemma3:1b\", temperature=0.1)\n",
    "\n",
    "chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm,\n",
    "    retriever,\n",
    "    combine_docs_chain_kwargs={\"prompt\": prompt},\n",
    "    return_source_documents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3fce2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9471/3057405367.py:9: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = chain({\"question\": q, \"chat_history\": chat_history})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ùì Question: If I'm not happy with my purchase, what is your refund policy and how do I start a return?\n",
      "üí¨ Answer: Okay, here‚Äôs the refund policy and return process, based on the provided context:\n",
      "\n",
      "**Refund Policy**\n",
      "\n",
      "We offer refunds for defective or unsatisfactory items within 30 days of delivery.  Here‚Äôs a breakdown:\n",
      "\n",
      "*   **Warehouse Receipt ‚Üí Inspection ‚â§ 3 Business Days:**  If the carrier scans your return label, authorization drops when our returns team receives the original item.\n",
      "*   **Return Window Exceptions:**\n",
      "    *   **Holiday Gift Purchases Made 1 Nov ‚Äì 31 Dec:**  The return period is extended to 31 January.\n",
      "    *   **‚ÄúFinal Sale‚Äù and Custom-Embroidered Items:** Returns are permitted for store credit.\n",
      "*   **Defect & Warranty Claims (12):**  If you have a defect or warranty claim, you must file it within 12 days of the return.\n",
      "\n",
      "**How to Start a Return**\n",
      "\n",
      "1.  **Scan Your Return Label:**  The carrier scans the return label when your item is returned.\n",
      "2.  **Authorization Drops:**  Our returns team receives the original item when authorization drops.\n",
      "3.  **Return Window Exceptions:**\n",
      "    *   **Holiday Gift Purchases Made 1 Nov ‚Äì 31 Dec:**  The return period is extended to 31 January.\n",
      "    *   **‚ÄúFinal Sale‚Äù and Custom-Embroidered Items:** Returns are permitted for store credit.\n",
      "4.  **Send the Item Back:**  Pack the item securely and send it back within 30 days of delivery.\n",
      "5.  **Refund:** We issue refunds the same day your return is scanned.\n",
      "\n",
      "**Where's my Klarna Statement?**\n",
      "\n",
      "Klarna emails the payment schedule shortly after order confirmation. If you don't receive it, log in at [Klarna website address]\n",
      "üìÑ Sources:\n",
      "   - data/Everstorm_Return_and_exchange_policy.pdf\n",
      "   - data/Everstorm_Return_and_exchange_policy.pdf\n",
      "   - data/Everstorm_Return_and_exchange_policy.pdf\n",
      "   - data/Everstorm_Return_and_exchange_policy.pdf\n",
      "   - data/Everstorm_Payment_refund_and_security.pdf\n",
      "   - data/Everstorm_Return_and_exchange_policy.pdf\n",
      "   - data/Everstorm_Return_and_exchange_policy.pdf\n",
      "   - data/Everstorm_Payment_refund_and_security.pdf\n",
      "\n",
      "‚ùì Question: How long will delivery take for a standard order, and where can I track my package once it ships?\n",
      "üí¨ Answer: A standard order typically takes 7.95 to 12 hours to be delivered, and you can track your package via a tracking link that is emailed upon label creation.  The tracking link is located at [https://live.everstorm.example/tracking](https://live.everstorm.example/tracking).\n",
      "üìÑ Sources:\n",
      "   - data/Everstorm_Shipping_and_Delivery_Policy.pdf\n",
      "   - data/Everstorm_Shipping_and_Delivery_Policy.pdf\n",
      "   - data/Everstorm_Shipping_and_Delivery_Policy.pdf\n",
      "   - data/Everstorm_Shipping_and_Delivery_Policy.pdf\n",
      "   - data/Everstorm_Shipping_and_Delivery_Policy.pdf\n",
      "   - data/Everstorm_Shipping_and_Delivery_Policy.pdf\n",
      "   - data/Everstorm_Shipping_and_Delivery_Policy.pdf\n",
      "   - data/Everstorm_Shipping_and_Delivery_Policy.pdf\n",
      "\n",
      "‚ùì Question: What's the quickest way to contact your support team, and what are your operating hours?\n",
      "üí¨ Answer: Within 30 minutes of order placement, you can contact logistics@everstorm.example. You can contact them via chat or log in at [Live chat](https://live.everstorm.example).  Your operating hours are 08:00‚Äì18:00 MT.\n",
      "üìÑ Sources:\n",
      "   - data/Everstorm_Return_and_exchange_policy.pdf\n",
      "   - data/Everstorm_Shipping_and_Delivery_Policy.pdf\n",
      "   - data/Everstorm_Shipping_and_Delivery_Policy.pdf\n",
      "   - data/Everstorm_Shipping_and_Delivery_Policy.pdf\n",
      "   - data/Everstorm_Return_and_exchange_policy.pdf\n",
      "   - data/Everstorm_Return_and_exchange_policy.pdf\n",
      "   - data/Everstorm_Payment_refund_and_security.pdf\n",
      "   - data/Everstorm_Return_and_exchange_policy.pdf\n"
     ]
    }
   ],
   "source": [
    "test_questions = [\n",
    "    \"If I'm not happy with my purchase, what is your refund policy and how do I start a return?\",\n",
    "    \"How long will delivery take for a standard order, and where can I track my package once it ships?\",\n",
    "    \"What's the quickest way to contact your support team, and what are your operating hours?\",\n",
    "]\n",
    "\n",
    "chat_history = []\n",
    "for q in test_questions:\n",
    "    result = chain({\"question\": q, \"chat_history\": chat_history})\n",
    "    print(\"\\n‚ùì Question:\", q)\n",
    "    print(\"üí¨ Answer:\", result[\"answer\"])\n",
    "    chat_history.append((q, result[\"answer\"]))\n",
    "    print(\"üìÑ Sources:\")\n",
    "    for doc in result[\"source_documents\"]:\n",
    "        print(f\"   - {doc.metadata['source']}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58892fae",
   "metadata": {},
   "source": [
    "### 6‚ÄØ-‚ÄØBuild the Streamlit UI (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8ecc42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Streamlit app code written to app.py\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat > app.py <<'PY'\n",
    "import pathlib, streamlit as st\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import Ollama\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "st.set_page_config(page_title=\"Customer Support Chatbot\", page_icon=\"ü§ñ\")\n",
    "st.title(\"Customer Support Chatbot ü§ñ\")\n",
    "\n",
    "@st.cache_resource\n",
    "def init_chain():\n",
    "    vectordb = FAISS.load_local(\n",
    "        \"faiss_index\",\n",
    "        HuggingFaceEmbeddings(model_name=\"thenlper/gte-small\"),\n",
    "        allow_dangerous_deserialization=True,\n",
    "    )\n",
    "    retriever = vectordb.as_retriever(search_kwargs={\"k\": 8})\n",
    "    llm = Ollama(model=\"gemma3:1b\", temperature=0.1)\n",
    "\n",
    "    memory = ConversationBufferMemory(\n",
    "        memory_key=\"chat_history\",\n",
    "        return_messages=True,\n",
    "    )\n",
    "\n",
    "    return ConversationalRetrievalChain.from_llm(\n",
    "        llm,\n",
    "        retriever,\n",
    "        memory=memory,\n",
    "    )\n",
    "\n",
    "chain = init_chain()\n",
    "\n",
    "if \"history\" not in st.session_state:\n",
    "    st.session_state.history = []\n",
    "\n",
    "question = st.chat_input(\"What is in your mind?\")\n",
    "if question:\n",
    "    with st.spinner(\"Thinking...\"):\n",
    "        response = chain(\n",
    "            {\n",
    "                \"question\": question,\n",
    "                \"chat_history\": st.session_state.history,   # <- supply it\n",
    "            }\n",
    "        )\n",
    "    st.session_state.history.append((question, response[\"answer\"]))\n",
    "\n",
    "\n",
    "for user, bot in reversed(st.session_state.history):\n",
    "    st.markdown(f\"**You:** {user}\")\n",
    "    st.markdown(f\"**Bot:** {bot}\")\n",
    "PY\n",
    "echo \"‚úÖ Streamlit app code written to app.py\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_demo_projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
